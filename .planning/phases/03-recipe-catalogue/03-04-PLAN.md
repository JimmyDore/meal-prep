---
phase: 03-recipe-catalogue
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.prod.yml
  - src/lib/env.ts
  - .github/workflows/deploy.yml
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Production app does not crash when PIPELINE_TOKEN is available in container environment"
    - "Non-pipeline routes do not require PIPELINE_TOKEN to function"
    - "Database migrations are applied automatically on every deploy"
  artifacts:
    - path: "docker-compose.prod.yml"
      provides: "PIPELINE_TOKEN passed to app container from host .env"
      contains: "PIPELINE_TOKEN"
    - path: "src/lib/env.ts"
      provides: "PIPELINE_TOKEN as optional server env var"
      contains: "optional"
    - path: ".github/workflows/deploy.yml"
      provides: "Migration step in deploy script before app restart"
      contains: "drizzle"
  key_links:
    - from: "docker-compose.prod.yml"
      to: "src/lib/env.ts"
      via: "PIPELINE_TOKEN env var passed from host to container to runtime validation"
      pattern: "PIPELINE_TOKEN"
    - from: ".github/workflows/deploy.yml"
      to: "drizzle/*.sql"
      via: "deploy script runs migrations against prod DB before restarting app"
      pattern: "migrate"
---

<objective>
Fix the 3 code-level root causes of the production 500 error on /recipes.

Purpose: The production app at mealprep.jimmydore.fr crashes at module load time because (1) PIPELINE_TOKEN is missing from the container environment, (2) env.ts validates it as required, and (3) the deploy script has no migration step. These are all local code fixes that must be pushed before VPS remediation.

Output: Three files modified and committed -- docker-compose.prod.yml, src/lib/env.ts, .github/workflows/deploy.yml
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/debug/prod-500-recipes.md
@.planning/phases/03-recipe-catalogue/03-UAT.md
@docker-compose.prod.yml
@src/lib/env.ts
@.github/workflows/deploy.yml
@drizzle/0001_crazy_husk.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix docker-compose.prod.yml and env.ts</name>
  <files>docker-compose.prod.yml, src/lib/env.ts</files>
  <action>
    TWO changes:

    1. **docker-compose.prod.yml** -- Add PIPELINE_TOKEN to the app service environment section:
       ```yaml
       environment:
         DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@db:5432/${POSTGRES_DB}
         NODE_ENV: production
         PIPELINE_TOKEN: ${PIPELINE_TOKEN:-}
       ```
       Use `${PIPELINE_TOKEN:-}` (with empty default) so docker-compose does not fail if the var is unset on the host. The app-level validation in env.ts will handle whether it's required or not.

    2. **src/lib/env.ts** -- Make PIPELINE_TOKEN optional so non-pipeline routes don't crash:
       Change:
       ```ts
       PIPELINE_TOKEN: z.string().min(1),
       ```
       To:
       ```ts
       PIPELINE_TOKEN: z.string().min(1).optional(),
       ```
       This is safe because PIPELINE_TOKEN is only consumed by the upload API route (`src/app/api/recipes/upload/route.ts`), which already checks the Authorization header against `env.PIPELINE_TOKEN`. If the var is undefined, the upload route will reject all requests (which is correct -- pipeline shouldn't run against a server without a token). But critically, ALL OTHER routes (like /recipes) will no longer crash at import time.

    **Why optional instead of a dummy default:** A dummy default would silently allow unauthorized uploads if someone forgets to set the real token. Optional means: no token = upload route rejects everything = secure by default.

    **Important:** Also verify the upload route handles undefined PIPELINE_TOKEN gracefully. Read `src/app/api/recipes/upload/route.ts` and confirm it compares `authorization === \`Bearer ${env.PIPELINE_TOKEN}\``. If PIPELINE_TOKEN is undefined, this comparison will be `Bearer undefined` which will never match any real token -- safe. If the route uses a different pattern, add a guard: `if (!env.PIPELINE_TOKEN) return NextResponse.json({ error: "Pipeline not configured" }, { status: 503 })`.
  </action>
  <verify>
    1. `pnpm tsc --noEmit` passes (env.PIPELINE_TOKEN is now `string | undefined`, verify no type errors)
    2. `pnpm biome ci .` passes
    3. `grep -n "PIPELINE_TOKEN" docker-compose.prod.yml` shows the new env var line
    4. `grep -n "optional" src/lib/env.ts` shows PIPELINE_TOKEN is optional
  </verify>
  <done>
    docker-compose.prod.yml passes PIPELINE_TOKEN to the app container. env.ts makes PIPELINE_TOKEN optional so non-pipeline routes work without it. Type-check and lint pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add migration step to deploy workflow</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Add an automated migration step to the deploy SSH script in `.github/workflows/deploy.yml`.

    The current deploy script is:
    ```bash
    cd /home/jimmydore/meal-prep
    git pull origin main
    docker compose -f docker-compose.prod.yml build
    docker compose -f docker-compose.prod.yml up -d
    docker image prune -f
    ```

    Change it to:
    ```bash
    cd /home/jimmydore/meal-prep
    git pull origin main
    docker compose -f docker-compose.prod.yml build
    docker compose -f docker-compose.prod.yml up -d
    sleep 5
    docker compose -f docker-compose.prod.yml exec -T db psql -U ${POSTGRES_USER} -d ${POSTGRES_DB} -c "CREATE TABLE IF NOT EXISTS drizzle_migrations (id SERIAL PRIMARY KEY, hash TEXT NOT NULL UNIQUE, created_at TIMESTAMP DEFAULT NOW());"
    for f in drizzle/[0-9]*.sql; do
      hash=$(sha256sum "$f" | cut -d' ' -f1)
      if ! docker compose -f docker-compose.prod.yml exec -T db psql -U ${POSTGRES_USER} -d ${POSTGRES_DB} -tAc "SELECT 1 FROM drizzle_migrations WHERE hash='$hash'" | grep -q 1; then
        echo "Applying migration: $f"
        docker compose -f docker-compose.prod.yml exec -T db psql -U ${POSTGRES_USER} -d ${POSTGRES_DB} < "$f"
        docker compose -f docker-compose.prod.yml exec -T db psql -U ${POSTGRES_USER} -d ${POSTGRES_DB} -c "INSERT INTO drizzle_migrations (hash) VALUES ('$hash');"
      else
        echo "Skipping already applied: $f"
      fi
    done
    docker image prune -f
    ```

    **Why this approach:**
    - Creates a simple `drizzle_migrations` tracking table (idempotent via IF NOT EXISTS)
    - Iterates over all migration SQL files in sorted order (the `[0-9]*.sql` glob naturally sorts `0000_`, `0001_`, etc.)
    - Uses SHA-256 hash of the file to detect if already applied (not filename, because filenames could be regenerated)
    - Each migration is applied directly via psql into the running db container
    - `sleep 5` ensures the db container is fully ready after `up -d`
    - The `-T` flag prevents psql from requiring a TTY (critical for CI)

    **Why NOT drizzle-kit migrate:** The production Docker image is a Next.js standalone build -- it does not have node_modules, drizzle-kit, or the drizzle config. Running drizzle-kit would require a separate Node.js container or installing deps on the VPS. Applying SQL directly via psql is simpler and more reliable.

    **Environment variables:** `${POSTGRES_USER}` and `${POSTGRES_DB}` are available on the VPS from the `.env` file that docker-compose.prod.yml reads. However, in the SSH script context they may not be loaded. Source the .env first:

    ```bash
    cd /home/jimmydore/meal-prep
    set -a && source .env && set +a
    git pull origin main
    ...
    ```

    Add `set -a && source .env && set +a` right after the `cd` line to export all .env vars into the shell session.
  </action>
  <verify>
    1. Read `.github/workflows/deploy.yml` and confirm the migration loop is present between `up -d` and `docker image prune`
    2. Confirm `set -a && source .env && set +a` is present to load env vars
    3. Confirm `sleep 5` is present after `up -d`
    4. `pnpm biome ci .` still passes (YAML file, no JS impact)
  </verify>
  <done>
    Deploy workflow applies all pending Drizzle migrations automatically via psql before pruning images. Migration tracking table prevents re-applying already-applied migrations. Future schema changes will auto-deploy.
  </done>
</task>

</tasks>

<verification>
1. `pnpm tsc --noEmit` passes with SKIP_ENV_VALIDATION unset (validates env.ts change is type-safe)
2. `pnpm biome ci .` passes
3. `grep "PIPELINE_TOKEN" docker-compose.prod.yml` returns the new line
4. `grep "optional" src/lib/env.ts` confirms PIPELINE_TOKEN is optional
5. `grep "drizzle_migrations" .github/workflows/deploy.yml` confirms migration step exists
</verification>

<success_criteria>
- docker-compose.prod.yml passes PIPELINE_TOKEN from host .env to app container
- env.ts validates PIPELINE_TOKEN as optional (non-pipeline routes work without it)
- deploy.yml applies pending SQL migrations automatically on every deploy
- All linting and type checks pass
- Code is committed and ready to push
</success_criteria>

<output>
After completion, create `.planning/phases/03-recipe-catalogue/03-04-SUMMARY.md`
</output>
