---
phase: 02-recipe-data-pipeline
plan: 05
type: execute
wave: 3
depends_on: ["02-02", "02-03", "02-04"]
files_modified:
  - scripts/pipeline/upload.ts
  - scripts/pipeline/lib/api-client.ts
autonomous: false

must_haves:
  truths:
    - "Running `tsx scripts/pipeline/upload.ts` reads enriched JSONL and uploads each recipe to the server API"
    - "Upload client authenticates with bearer token from PIPELINE_TOKEN env var"
    - "Upload skips recipes already in the database (checks by jowId before uploading or handles 201 idempotently)"
    - "Upload logs progress with success/skip/fail counters"
    - "End-to-end pipeline (scrape -> enrich -> upload) produces recipes consultable in the database"
  artifacts:
    - path: "scripts/pipeline/upload.ts"
      provides: "Main upload entry point"
      min_lines: 40
    - path: "scripts/pipeline/lib/api-client.ts"
      provides: "HTTP client for recipe upload API"
      min_lines: 30
  key_links:
    - from: "scripts/pipeline/upload.ts"
      to: "data/enriched/jow-recipes-enriched.jsonl"
      via: "readJsonl input"
      pattern: "readJsonl.*enriched"
    - from: "scripts/pipeline/lib/api-client.ts"
      to: "/api/recipes/upload"
      via: "fetch POST with bearer token"
      pattern: "fetch.*recipes/upload|Bearer"
    - from: "scripts/pipeline/upload.ts"
      to: "scripts/pipeline/lib/api-client.ts"
      via: "import uploadRecipe function"
      pattern: "import.*api-client"
---

<objective>
Build the upload client that reads enriched recipes from JSONL and sends them to the server's POST /api/recipes/upload endpoint. Then verify the complete pipeline works end-to-end: scrape a few recipes, enrich them, upload them, and confirm they are in the database.

Purpose: This is the final pipeline step that connects the local tooling to the server. Without it, enriched recipes stay in local files and never reach the database. The end-to-end verification proves the entire Phase 2 pipeline works.
Output: upload.ts entry point + api-client.ts (HTTP client) + end-to-end verification
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-recipe-data-pipeline/02-RESEARCH.md
@.planning/phases/02-recipe-data-pipeline/02-CONTEXT.md
@.planning/phases/02-recipe-data-pipeline/02-01-SUMMARY.md
@.planning/phases/02-recipe-data-pipeline/02-02-SUMMARY.md
@.planning/phases/02-recipe-data-pipeline/02-03-SUMMARY.md
@.planning/phases/02-recipe-data-pipeline/02-04-SUMMARY.md
@scripts/pipeline/lib/types.ts
@scripts/pipeline/lib/schemas.ts
@scripts/pipeline/lib/jsonl.ts
@scripts/pipeline/lib/logger.ts
@src/app/api/recipes/upload/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create upload client and upload entry point</name>
  <files>
    scripts/pipeline/lib/api-client.ts
    scripts/pipeline/upload.ts
  </files>
  <action>
1. **Create `scripts/pipeline/lib/api-client.ts`** -- HTTP client for upload:

   a) `createApiClient(baseUrl: string, token: string): ApiClient`
      - Returns an object with methods for interacting with the API
      - Stores baseUrl and token for reuse

   b) `uploadRecipe(recipe: EnrichedRecipe): Promise<{ id: string } | { error: string }>`
      - Constructs the upload payload from EnrichedRecipe:
        - Map enrichedIngredients to the ingredient format expected by the API (name, quantity, unit, originalText, proteinPer100g, carbsPer100g, fatPer100g, caloriesPer100g, confidence)
        - Include all recipe fields (jowId, title, description, imageUrl, jowUrl, cookTimeMin, prepTimeMin, totalTimeMin, difficulty, instructions, nutriScore, rating, ratingCount, cuisine, category, originalPortions, jowNutritionPerServing)
        - Extract tags from recipe if available (or default to empty array)
      - POST to `{baseUrl}/api/recipes/upload` with:
        - `Content-Type: application/json`
        - `Authorization: Bearer {token}`
        - JSON body
      - Parse response: if 201, return `{ id }`. If 400/401/500, return `{ error: response status + body }`
      - Handle network errors (fetch throws) -- return `{ error: "Network error: {message}" }`

2. **Create `scripts/pipeline/upload.ts`** -- Main upload entry point:

   - Executable via `tsx scripts/pipeline/upload.ts`
   - Loads dotenv/config for PIPELINE_TOKEN
   - Creates logger with step name "upload"
   - Creates API client with:
     - `baseUrl`: from `--url` CLI flag or default `http://localhost:3000`
     - `token`: from `PIPELINE_TOKEN` env var (fail fast if missing)
   - **Input:** reads `data/enriched/jow-recipes-enriched.jsonl` via `readJsonl<EnrichedRecipe>()`
   - **Processing loop:** for each enriched recipe:
     - Call `apiClient.uploadRecipe(recipe)`
     - If success: increment success counter, log recipe title
     - If error: log error, increment fail counter, continue (don't abort)
     - Log progress every 25 recipes: "Uploaded N/Total (Y failed)"
   - **Summary:** prints final stats via logger.summary()

   **CLI flags:**
   - `--url URL` to override API base URL (default: `http://localhost:3000`)
   - `--limit N` to upload only N recipes
   - `--input path` to override input JSONL path
   - Parse with simple `process.argv` checks

   **Note on idempotency:** The API endpoint handles upsert, so re-uploading is safe. The upload script does NOT need to track "already uploaded" separately -- just re-upload. The API returns 201 regardless (upsert).
  </action>
  <verify>
    - Both files compile: `pnpm tsc --noEmit`
    - With dev server running (`pnpm dev`): `tsx scripts/pipeline/upload.ts --limit 3` uploads 3 recipes from enriched JSONL
    - Each upload returns 201 with a recipe ID
    - Re-running upload with same data succeeds (idempotent upsert)
  </verify>
  <done>
    Upload script reads enriched JSONL, sends each recipe to POST /api/recipes/upload with bearer token auth. Handles errors per-recipe without aborting. Supports --url for targeting different environments, --limit for testing. API client properly maps EnrichedRecipe to the upload payload format.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: End-to-end pipeline verification</name>
  <what-built>
    Complete pipeline: scrape -> enrich -> upload. Three independent scripts that can be run sequentially to move Jow recipes from the website into the database.
  </what-built>
  <how-to-verify>
    The executor will have already run individual steps during previous plans. For end-to-end verification:

    1. Ensure dev server is running: `pnpm dev`
    2. Verify data exists in JSONL files:
       - `wc -l data/scraped/jow-recipes.jsonl` (should show scraped recipe count)
       - `wc -l data/enriched/jow-recipes-enriched.jsonl` (should show enriched recipe count)
    3. Verify recipes in database: open `pnpm db:studio` and check:
       - Recipes table has entries with jowId, title, and enriched columns populated
       - Ingredients table has entries with macro values (proteinPer100g, etc.)
       - recipe_ingredients junction has entries linking recipes to ingredients
       - Tags table has entries (if recipes had tags)
    4. Verify a specific recipe end-to-end: pick one recipe from the JSONL, verify its data matches what's in the database
    5. Verify idempotency: re-run `tsx scripts/pipeline/upload.ts --limit 3` and confirm no duplicates created

    If recipes are visible in the database with correct macros and ingredients, the pipeline works.
  </how-to-verify>
  <resume-signal>Type "approved" if recipes are in the database with correct data, or describe issues</resume-signal>
</task>

</tasks>

<verification>
- `tsx scripts/pipeline/upload.ts --limit 5` uploads 5 recipes successfully
- Recipes are queryable in database with all fields (title, description, macros, ingredients, tags)
- Upsert idempotency: re-upload same recipes produces no duplicates
- Pipeline runs in sequence: scrape --limit 5 -> enrich --limit 5 -> upload --limit 5
- `pnpm tsc --noEmit` passes
- `pnpm test run` passes
</verification>

<success_criteria>
1. Upload script reads enriched JSONL and POSTs to /api/recipes/upload
2. Bearer token authentication works end-to-end
3. Recipes with ingredients and macros are persisted in the database
4. Pipeline is resumable and idempotent at every step
5. At least 3-5 recipes verified end-to-end (scrape -> enrich -> upload -> DB query)
</success_criteria>

<output>
After completion, create `.planning/phases/02-recipe-data-pipeline/02-05-SUMMARY.md`
</output>
