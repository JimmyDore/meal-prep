---
phase: 02-recipe-data-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/db/schema/recipes.ts
  - src/db/schema/ingredients.ts
  - src/lib/env.ts
  - scripts/pipeline/lib/schemas.ts
  - scripts/pipeline/lib/types.ts
  - scripts/pipeline/lib/jsonl.ts
  - scripts/pipeline/lib/logger.ts
autonomous: true

must_haves:
  truths:
    - "Schema migration adds all new columns to recipes table (description, prepTimeMin, totalTimeMin, difficulty, instructions, nutriScore, rating, ratingCount, cuisine, category, jowNutritionPerServing)"
    - "Ingredients table has unique constraint on name for upsert support"
    - "PIPELINE_TOKEN env var is defined in env validation"
    - "Pipeline Zod schemas validate scraped and enriched recipe data"
    - "JSONL read/write utilities work with streaming async generators"
  artifacts:
    - path: "src/db/schema/recipes.ts"
      provides: "Extended recipes table with all Jow data columns"
      contains: "description"
    - path: "src/db/schema/ingredients.ts"
      provides: "Ingredients table with unique name constraint"
      contains: "unique"
    - path: "scripts/pipeline/lib/schemas.ts"
      provides: "Zod validation schemas for pipeline data"
      exports: ["scrapedRecipeSchema", "enrichedRecipeSchema"]
    - path: "scripts/pipeline/lib/jsonl.ts"
      provides: "JSONL streaming read/write utilities"
      exports: ["readJsonl", "writeJsonl", "appendJsonl"]
  key_links:
    - from: "scripts/pipeline/lib/schemas.ts"
      to: "src/sources/types.ts"
      via: "Type alignment with RawRecipe/RawIngredient"
      pattern: "import.*sources/types"
---

<objective>
Extend the database schema with all columns needed for rich Jow recipe data, add unique constraint on ingredients.name for upsert, add PIPELINE_TOKEN to env validation, and create the shared pipeline infrastructure (Zod schemas, types, JSONL utilities, logger) that all pipeline steps depend on.

Purpose: This is the foundation layer -- schema migration must happen before the API endpoint can accept rich data, and shared pipeline libraries must exist before scraper/enricher/uploader can be built.
Output: Extended DB schema with migration, pipeline scaffolding in scripts/pipeline/lib/
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-recipe-data-pipeline/02-RESEARCH.md
@.planning/phases/02-recipe-data-pipeline/02-CONTEXT.md
@src/db/schema/recipes.ts
@src/db/schema/ingredients.ts
@src/db/schema/common.ts
@src/db/schema/index.ts
@src/sources/types.ts
@src/lib/env.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend database schema and generate migration</name>
  <files>
    src/db/schema/recipes.ts
    src/db/schema/ingredients.ts
    src/lib/env.ts
  </files>
  <action>
1. **Extend recipes table** in `src/db/schema/recipes.ts` -- add these columns using existing patterns (import from drizzle-orm/pg-core):
   - `description: text()` -- recipe description
   - `prepTimeMin: integer()` -- prep time separate from cookTimeMin
   - `totalTimeMin: integer()` -- total time (may differ from prep+cook)
   - `difficulty: text()` -- "Tres facile", "Facile", etc.
   - `instructions: jsonb()` -- array of step objects, import `jsonb` from drizzle-orm/pg-core
   - `nutriScore: text()` -- A/B/C/D/E
   - `rating: real()` -- aggregate rating, import `real` from drizzle-orm/pg-core
   - `ratingCount: integer()` -- number of reviews
   - `cuisine: text()` -- "Indienne", "Francaise", etc.
   - `category: text()` -- recipe category
   - `jowNutritionPerServing: jsonb()` -- original Jow per-serving nutrition for cross-validation

2. **Add unique constraint on ingredients.name** in `src/db/schema/ingredients.ts`:
   - Change `name: text().notNull()` to `name: text().unique().notNull()` to enable upsert by ingredient name.

3. **Add PIPELINE_TOKEN to env validation** in `src/lib/env.ts`:
   - Add `PIPELINE_TOKEN: z.string().min(1)` to the `server` object
   - Add `PIPELINE_TOKEN: process.env.PIPELINE_TOKEN` to `runtimeEnv`
   - This is the bearer token for the upload API endpoint

4. **Generate and apply migration:**
   - Run `pnpm db:generate` to create the SQL migration file
   - Run `pnpm db:push` to apply to dev DB (Docker Compose Postgres)
   - Verify no errors

Note: Do NOT modify seed.ts -- existing seed data remains compatible with schema additions (new columns are nullable).
  </action>
  <verify>
    - `pnpm db:generate` succeeds and creates a new migration file in drizzle/
    - `pnpm db:push` applies schema to dev DB without errors
    - `pnpm tsc --noEmit` passes (no type errors)
    - `pnpm test run` passes (existing tests still green)
  </verify>
  <done>
    Recipes table has 11 new columns (description through jowNutritionPerServing). Ingredients.name has unique constraint. PIPELINE_TOKEN is in env validation. Migration file exists in drizzle/ directory.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create pipeline scaffolding (schemas, types, JSONL utils, logger)</name>
  <files>
    scripts/pipeline/lib/schemas.ts
    scripts/pipeline/lib/types.ts
    scripts/pipeline/lib/jsonl.ts
    scripts/pipeline/lib/logger.ts
  </files>
  <action>
1. **Create directory structure:**
   - `scripts/pipeline/lib/`
   - `scripts/pipeline/prompts/`
   - `data/scraped/`
   - `data/enriched/`
   - Add `data/` to `.gitignore` (pipeline output should not be committed)

2. **Create `scripts/pipeline/lib/types.ts`** -- Pipeline-specific types:
   - `ScrapedRecipe`: extends concept from RawRecipe but with all Jow fields (title, description, jowId, jowUrl, imageUrl, cookTimeMin, prepTimeMin, totalTimeMin, difficulty, instructions as string[], nutriScore, rating, ratingCount, cuisine, category, originalPortions, ingredients as ScrapedIngredient[], jowNutritionPerServing as object with calories/fat/carbs/protein/fiber numbers)
   - `ScrapedIngredient`: name, quantity (number | null), unit (string | null), originalText (string)
   - `EnrichedRecipe`: extends ScrapedRecipe with `enrichedIngredients` array where each has name + proteinPer100g + carbsPer100g + fatPer100g + caloriesPer100g + confidence ("high"|"medium"|"low")
   - `EnrichedIngredient`: name, proteinPer100g, carbsPer100g, fatPer100g, caloriesPer100g, confidence
   - `PipelineStats`: success (number), skipped (number), failed (number), total (number)

3. **Create `scripts/pipeline/lib/schemas.ts`** -- Zod validation schemas:
   - `scrapedIngredientSchema`: validates ScrapedIngredient fields
   - `scrapedRecipeSchema`: validates ScrapedRecipe with `.array()` for ingredients, `.min(1)` on ingredients
   - `enrichedIngredientSchema`: validates macros with bounds (protein/carbs/fat: 0-100 per 100g, calories: 0-900 per 100g)
   - `enrichedRecipeSchema`: extends scraped with enrichedIngredients array
   - Export all schemas and their inferred types

4. **Create `scripts/pipeline/lib/jsonl.ts`** -- JSONL streaming utilities:
   - `readJsonl<T>(path: string): AsyncGenerator<T>` -- uses `readline.createInterface` with `createReadStream`, yields parsed JSON objects line by line, skips empty lines
   - `writeJsonl<T>(path: string, items: T[]): Promise<void>` -- writes all items to file (overwrites)
   - `appendJsonl<T>(path: string, item: T): Promise<void>` -- appends single item to file
   - `countLines(path: string): Promise<number>` -- counts lines in JSONL file for progress reporting

5. **Create `scripts/pipeline/lib/logger.ts`** -- Simple pipeline logger:
   - `createLogger(step: string): Logger` -- returns logger instance with methods: info, warn, error, summary
   - Logs to both console (summary format: step name + counters) and a log file (`data/{step}-{timestamp}.log`)
   - `summary(stats: PipelineStats)` -- prints formatted summary (success/skip/fail/total counts)

Note: Use `import` from "node:fs", "node:readline", "node:path" -- Node built-in modules with node: prefix as is standard practice.
  </action>
  <verify>
    - All 4 files exist and compile: `npx tsc --noEmit --skipLibCheck scripts/pipeline/lib/schemas.ts scripts/pipeline/lib/types.ts scripts/pipeline/lib/jsonl.ts scripts/pipeline/lib/logger.ts` OR verify via `pnpm tsc --noEmit` if tsconfig includes scripts/
    - `data/` directory exists and is in `.gitignore`
    - `scripts/pipeline/prompts/` directory exists
    - Zod schemas can validate a sample object without errors (quick inline test or write a small test)
  </verify>
  <done>
    Pipeline scaffolding complete: types.ts defines ScrapedRecipe/EnrichedRecipe/PipelineStats, schemas.ts has Zod validators with macro bounds, jsonl.ts has async streaming read/write/append/count, logger.ts has step-aware logging with file output and summary. data/ and prompts/ directories ready.
  </done>
</task>

</tasks>

<verification>
- `pnpm tsc --noEmit` passes (schema changes + new pipeline files compile)
- `pnpm test run` passes (existing tests unbroken by schema changes)
- Migration file exists in `drizzle/` directory
- `scripts/pipeline/lib/` contains schemas.ts, types.ts, jsonl.ts, logger.ts
- `data/` is gitignored
</verification>

<success_criteria>
1. Database schema extended with all 11 new recipe columns + ingredients.name unique constraint
2. Migration generated and applied to dev DB
3. PIPELINE_TOKEN in env validation
4. Pipeline shared library (types, schemas, JSONL utils, logger) compiles and is ready for use by scraper/enricher/uploader
</success_criteria>

<output>
After completion, create `.planning/phases/02-recipe-data-pipeline/02-01-SUMMARY.md`
</output>
