---
phase: 02-recipe-data-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/pipeline/scrape.ts
  - scripts/pipeline/lib/jow-scraper.ts
  - scripts/pipeline/lib/jow-parser.ts
autonomous: true

must_haves:
  truths:
    - "Running `tsx scripts/pipeline/scrape.ts` discovers all recipe URLs from Jow sitemap and scrapes recipe details to JSONL"
    - "Scraper respects 1-2 second rate limiting between requests"
    - "Scraper extracts all available data (title, description, ingredients, instructions, nutrition, ratings, difficulty, etc.)"
    - "Scraper detects and skips already-scraped recipes on re-run (resumability via JSONL)"
    - "Scraper logs progress with success/skip/fail counters"
  artifacts:
    - path: "scripts/pipeline/scrape.ts"
      provides: "Main scrape entry point"
      min_lines: 30
    - path: "scripts/pipeline/lib/jow-scraper.ts"
      provides: "Playwright scraping logic with sitemap discovery and detail scraping"
      min_lines: 80
    - path: "scripts/pipeline/lib/jow-parser.ts"
      provides: "Parse __NEXT_DATA__ and JSON-LD into ScrapedRecipe"
      min_lines: 50
  key_links:
    - from: "scripts/pipeline/scrape.ts"
      to: "scripts/pipeline/lib/jow-scraper.ts"
      via: "import scraper functions"
      pattern: "import.*jow-scraper"
    - from: "scripts/pipeline/lib/jow-scraper.ts"
      to: "scripts/pipeline/lib/jow-parser.ts"
      via: "import parser for HTML extraction"
      pattern: "import.*jow-parser"
    - from: "scripts/pipeline/scrape.ts"
      to: "data/scraped/jow-recipes.jsonl"
      via: "JSONL output via appendJsonl"
      pattern: "appendJsonl|writeJsonl"
---

<objective>
Build the Jow.fr scraper that discovers all recipe URLs via sitemap pages, then scrapes each recipe detail page using Playwright, extracts structured data from __NEXT_DATA__ and JSON-LD, and writes results to JSONL. Supports resumability (skips already-scraped recipes) and polite rate limiting.

Purpose: This is the data ingestion step -- without scraped recipes, there's nothing to enrich or upload. The scraper must handle ~3,214 recipes reliably with resumability.
Output: scrape.ts entry point + jow-scraper.ts (Playwright logic) + jow-parser.ts (HTML parsing)
</objective>

<execution_context>
@/Users/jimmydore/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jimmydore/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-recipe-data-pipeline/02-RESEARCH.md
@.planning/phases/02-recipe-data-pipeline/02-CONTEXT.md
@.planning/phases/02-recipe-data-pipeline/02-01-SUMMARY.md
@scripts/pipeline/lib/types.ts
@scripts/pipeline/lib/schemas.ts
@scripts/pipeline/lib/jsonl.ts
@scripts/pipeline/lib/logger.ts
@src/sources/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Playwright + Cheerio and create parser module</name>
  <files>
    scripts/pipeline/lib/jow-parser.ts
  </files>
  <action>
1. **Install dependencies:**
   - `pnpm add playwright cheerio`
   - `npx playwright install chromium` (installs only Chromium browser, not Firefox/WebKit)

2. **Create `scripts/pipeline/lib/jow-parser.ts`** -- Parser module with fallback chain:

   Export these functions:

   a) `parseNextDataRecipe(nextData: unknown): ScrapedRecipe | null`
      - Navigates `props.pageProps` to find the recipe object
      - Extracts: title, description, jowId (from URL path -- last 16+ alphanumeric chars), jowUrl, imageUrl, cookTimeMin, prepTimeMin, totalTimeMin, difficulty, instructions (array of step text strings), nutriScore, rating, ratingCount, cuisine, category, originalPortions
      - Extracts ingredients: map each to ScrapedIngredient (name, quantity, unit, originalText)
      - Extracts jowNutritionPerServing from nutrition data if available (calories, fat, carbs, protein, fiber as numbers)
      - Returns null if required fields (title, jowId) are missing
      - Log a warning with `console.warn` for any fields that could not be extracted (non-fatal)

   b) `parseJsonLdRecipe(jsonLd: unknown): ScrapedRecipe | null`
      - Parses Schema.org Recipe format from JSON-LD
      - Maps JSON-LD fields to ScrapedRecipe: name->title, description, prepTime/cookTime/totalTime (parse ISO 8601 PT durations to minutes), recipeYield->originalPortions, recipeIngredient->ingredients (parse text format), recipeInstructions->instructions, nutrition->jowNutritionPerServing, aggregateRating->rating/ratingCount, keywords->tags, recipeCuisine->cuisine, recipeCategory->category
      - Extract jowId from the page URL (passed as parameter) -- last segment after last dash
      - Returns null if required fields missing

   c) `parseIsoDuration(iso: string): number | null`
      - Parses ISO 8601 duration like "PT30M", "PT1H15M", "PT45M" to minutes
      - Returns null if unparseable

   d) `extractJowId(url: string): string`
      - Extracts the 16+ char alphanumeric ID from Jow URL path
      - Pattern: last segment of path, after the last dash: `/recipes/poulet-au-curry-89y06dxjhfua0twu16x5` -> `89y06dxjhfua0twu16x5`

   Use Cheerio only if needed for HTML string parsing. The primary data sources are JSON objects already parsed from the page.

   **IMPORTANT:** During initial scraping runs, the exact __NEXT_DATA__ structure is unknown at the field-path level. Design `parseNextDataRecipe` defensively:
   - `console.log` the first scraped __NEXT_DATA__ object to map all available fields
   - Use optional chaining extensively (`data?.props?.pageProps?.recipe?.title`)
   - The parser will likely need iteration after seeing real data -- this is expected
  </action>
  <verify>
    - `pnpm list playwright cheerio` shows both installed
    - `npx playwright install chromium` completed (or chromium was already installed)
    - File compiles: verified via `pnpm tsc --noEmit` or inline TypeScript check
    - `extractJowId("https://jow.fr/recipes/poulet-au-curry-89y06dxjhfua0twu16x5")` returns `"89y06dxjhfua0twu16x5"`
    - `parseIsoDuration("PT30M")` returns `30`, `parseIsoDuration("PT1H15M")` returns `75`
  </verify>
  <done>
    jow-parser.ts exports parseNextDataRecipe, parseJsonLdRecipe, parseIsoDuration, extractJowId. Parser handles both __NEXT_DATA__ and JSON-LD fallback. ISO duration parsing works for PT format. Playwright and Cheerio are installed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Playwright scraper and scrape entry point</name>
  <files>
    scripts/pipeline/lib/jow-scraper.ts
    scripts/pipeline/scrape.ts
  </files>
  <action>
1. **Create `scripts/pipeline/lib/jow-scraper.ts`** -- Playwright scraping logic:

   a) `createBrowser(): Promise<{ browser: Browser, context: BrowserContext }>`
      - Launches Chromium in headless mode
      - Sets user agent: `"MealPrepBot/1.0 (personal project, polite scraping)"`
      - Returns browser and context for reuse

   b) `discoverRecipeUrls(context: BrowserContext, logger: Logger): Promise<string[]>`
      - Iterates over letters a-z: navigates to `https://jow.fr/site-map/recipes/letter-{letter}`
      - For each page: extracts `__NEXT_DATA__` JSON, parses `groupRecipes` array to get recipe paths
      - Rate limits: `await delay(1500)` between each letter page (26 requests total)
      - Logs progress: "Discovered N recipes from letter X"
      - Returns array of full recipe URLs (prepend `https://jow.fr` to paths)
      - If a letter page fails, log warning and continue (don't abort entire discovery)

   c) `scrapeRecipeDetail(page: Page, url: string): Promise<ScrapedRecipe | null>`
      - Navigates to recipe URL with `waitUntil: "domcontentloaded"`
      - Attempts extraction in order (fallback chain):
        1. Extract `__NEXT_DATA__` via `page.evaluate()` -> pass to `parseNextDataRecipe()`
        2. Extract JSON-LD via `page.evaluate()` (find `script[type="application/ld+json"]` with @type "Recipe") -> pass to `parseJsonLdRecipe()`
      - Validates result with `scrapedRecipeSchema.safeParse()`
      - Returns null on failure (log the error, don't throw)

   d) `delay(ms: number): Promise<void>` -- simple setTimeout wrapper

2. **Create `scripts/pipeline/scrape.ts`** -- Main scrape entry point:

   - Executable via `tsx scripts/pipeline/scrape.ts`
   - Creates logger with step name "scrape"
   - Creates browser via `createBrowser()`
   - **Phase 1 - Discovery:** calls `discoverRecipeUrls()` to get all recipe URLs
   - **Resumability check:** reads existing `data/scraped/jow-recipes.jsonl` if it exists, builds a Set of already-scraped jowIds (extract from each line), filters out URLs whose jowId is already present
   - **Phase 2 - Detail scraping:** iterates over remaining URLs:
     - Opens new page, calls `scrapeRecipeDetail(page, url)`
     - If successful: appends to `data/scraped/jow-recipes.jsonl` via `appendJsonl()`
     - If failed: log warning, increment fail counter
     - Rate limits: `await delay(1500)` between each recipe (1.5 seconds)
     - Closes page after each recipe (don't leak pages)
     - Logs progress every 50 recipes: "Scraped N/Total (X skipped, Y failed)"
   - **Cleanup:** closes browser
   - **Summary:** prints final stats via logger.summary()
   - Wraps everything in try/catch/finally to ensure browser cleanup on error

   **CLI flags (optional but nice):**
   - `--limit N` to scrape only N recipes (for testing)
   - `--dry-run` to do discovery only without detail scraping
   - Parse with simple `process.argv` checks (no library needed)

   **Script should handle graceful interruption:** the JSONL append-only pattern means progress is saved even if the script is killed mid-run. The next run picks up where it left off.
  </action>
  <verify>
    - `tsx scripts/pipeline/scrape.ts --dry-run` runs successfully: discovers recipe URLs from sitemap, prints count, exits without detail scraping
    - `tsx scripts/pipeline/scrape.ts --limit 3` scrapes 3 recipes, creates `data/scraped/jow-recipes.jsonl` with 3 lines
    - Each JSONL line is valid JSON with at minimum: jowId, title, jowUrl fields
    - Re-running `tsx scripts/pipeline/scrape.ts --limit 3` skips already-scraped recipes (0 new, 3 skipped)
    - `pnpm tsc --noEmit` passes
  </verify>
  <done>
    Scraper discovers all ~3,214 recipe URLs from Jow sitemap, scrapes recipe details with Playwright using __NEXT_DATA__ / JSON-LD fallback, writes JSONL output, supports resumability (skips existing), rate limits at 1.5s between requests, and provides progress logging. --limit and --dry-run flags for testing.
  </done>
</task>

</tasks>

<verification>
- Playwright and Cheerio installed (`pnpm list playwright cheerio`)
- `tsx scripts/pipeline/scrape.ts --dry-run` outputs recipe URL count (~3200)
- `tsx scripts/pipeline/scrape.ts --limit 5` creates JSONL with 5 scraped recipes
- Each recipe has: jowId, title, jowUrl, imageUrl, ingredients (array), originalPortions
- Re-run with --limit 5 shows 0 new, 5 skipped (resumability works)
- `pnpm tsc --noEmit` passes
</verification>

<success_criteria>
1. Scraper discovers recipe URLs from all 26 Jow sitemap letter pages
2. Scraper extracts rich recipe data (title, description, ingredients, instructions, nutrition, ratings, difficulty, etc.) from detail pages
3. Output is valid JSONL in data/scraped/jow-recipes.jsonl
4. Resumability: re-run skips already-scraped recipes
5. Rate limiting: 1.5s delay between requests
</success_criteria>

<output>
After completion, create `.planning/phases/02-recipe-data-pipeline/02-02-SUMMARY.md`
</output>
